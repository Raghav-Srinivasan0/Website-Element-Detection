{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I will process my data and run a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "\n",
    "import cv2 \n",
    "\n",
    "from get_coords import Data\n",
    "\n",
    "# do batch prediction function DONE\n",
    "# mix features together DONE\n",
    "# add color features DONE\n",
    "# mean square error for regression DONE\n",
    "# get started writing \n",
    "# get results 1/2 DONE\n",
    "\n",
    "# Features to try: DONE\n",
    "# 1) Raw Position Values DONE\n",
    "# Find how they did visual complexity \n",
    "# Think about why some features are doing better\n",
    "# try cross validation \n",
    "# reshuffle training and testing data\n",
    "# try removing some percent of training and test on different testing sets and train on different training sets\n",
    "# train should be different from test ALWAYS\n",
    "\n",
    "# start writing related works \n",
    "# start writing methodology\n",
    "# write indroduction\n",
    "# write like other papers that I have read\n",
    "\n",
    "# try to get positive results\n",
    "\n",
    "# arxiv for posting negative result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\woprg/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-9-27 Python-3.10.4 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7031701 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "classes = ['button', 'field', 'heading', 'iframe', 'image', 'label', 'link', 'text']\n",
    "\n",
    "directory = \"dataset/images/english/\"\n",
    "#directory = \"small_test/\"\n",
    "\n",
    "SHUFFLE = False\n",
    "\n",
    "data = Data(\n",
    "    directory=directory,\n",
    "    shuffle=SHUFFLE\n",
    "    )\n",
    "\n",
    "raw_data = data.filter_confidence()\n",
    "image_names = list(raw_data.keys())\n",
    "image_path = data.images()\n",
    "images = {}\n",
    "\n",
    "\n",
    "for i in range(len(image_path)):\n",
    "    images[image_names[i]] = cv2.imread(image_path[i])\n",
    "\n",
    "def raw():\n",
    "    rawData = {}\n",
    "    temp = list(raw_data.values())\n",
    "    for i in range(len(temp)):\n",
    "        temp_rawData = []\n",
    "        for element in temp[i]:\n",
    "            e = []\n",
    "            e.append(float((float(element[0])+float(element[2]))/2))\n",
    "            e.append(float((float(element[1])+float(element[3]))/2))\n",
    "            e.append(float(element[5]))\n",
    "            temp_rawData.extend(e)\n",
    "        rawData[int(image_names[i])] = temp_rawData\n",
    "    sizes = []\n",
    "    for i in range(len(rawData)):\n",
    "        sizes.append(len(rawData[i]))\n",
    "    max_size = max(sizes)\n",
    "    for i in range(len(rawData)):\n",
    "        if len(rawData[i]) < max_size:\n",
    "            for j in range(max_size - len(rawData[i])):\n",
    "                rawData[i].append(0)\n",
    "    return rawData\n",
    "\n",
    "#print(raw())\n",
    "\n",
    "\n",
    "def colorfulness():\n",
    "    colorfulness = {}\n",
    "    for i in range(len(list(images.values()))):\n",
    "        current_colorfulness = 0\n",
    "        count = 0\n",
    "        temp_image = list(images.values())[i] \n",
    "        hsv = cv2.cvtColor(temp_image, cv2.COLOR_BGR2HSV)\n",
    "        for row in range(len(hsv)):\n",
    "            for column in range(len(hsv[row])):\n",
    "                current_colorfulness = current_colorfulness + hsv[row][column][1] * hsv[row][column][2]\n",
    "                count = count + 1\n",
    "        result = current_colorfulness/count\n",
    "        colorfulness[int(list(images.keys())[i])] = result\n",
    "    return colorfulness\n",
    "\n",
    "def avgPos():\n",
    "    temp = list(raw_data.values())\n",
    "    avg_element_position = {}\n",
    "    for i in range(len(temp)):\n",
    "        avg_pixel_position_for_class = {}\n",
    "        count = {}\n",
    "        for element in temp[i]:\n",
    "            if avg_pixel_position_for_class.get(int(element[5])) == None:\n",
    "                avg_pixel_position_for_class[int(element[5])] = ((float(element[0])+float(element[2]))/2,(float(element[1])+float(element[3]))/2)\n",
    "                count[int(element[5])] = 1\n",
    "            else:\n",
    "                count[int(element[5])] += 1\n",
    "                avg_pixel_position_for_class[int(element[5])] = ((((float(element[0])+float(element[2]))/2)+avg_pixel_position_for_class[int(element[5])][0])/count[int(element[5])],(((float(element[1])+float(element[3]))/2)+avg_pixel_position_for_class[int(element[5])][0])/count[int(element[5])])\n",
    "                \n",
    "        avg_element_position[int(image_names[i])] = avg_pixel_position_for_class\n",
    "        #print(count)\n",
    "\n",
    "    return avg_element_position\n",
    "\n",
    "def freqQuad():\n",
    "    class_per_quadrant = {}\n",
    "    temp = list(raw_data.values())\n",
    "    for i in range(len(temp)):\n",
    "        temp_data_holder = [{},{},{},{}]\n",
    "        temp_data_holder_2 = []\n",
    "        temp_image_name = image_names[i]\n",
    "        temp_image = cv2.imread(directory + temp_image_name + '.png')\n",
    "        w = temp_image.shape[0]\n",
    "        h = temp_image.shape[1]\n",
    "        for element in temp[i]:\n",
    "            center_X = (element[0]+element[2])/2\n",
    "            center_Y = (element[1]+element[3])/2\n",
    "            if center_X >= w/2 and center_Y >= h/2:\n",
    "                if temp_data_holder[0].get(int(element[5])) == None:\n",
    "                    temp_data_holder[0][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[0][int(element[5])] = temp_data_holder[0][int(element[5])] + 1\n",
    "            if center_X < w/2 and center_Y >= h/2:\n",
    "                if temp_data_holder[1].get(int(element[5])) == None:\n",
    "                    temp_data_holder[1][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[1][int(element[5])] = temp_data_holder[1][int(element[5])] + 1\n",
    "            if center_X < w/2 and center_Y < h/2:\n",
    "                if temp_data_holder[2].get(int(element[5])) == None:\n",
    "                    temp_data_holder[2][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[2][int(element[5])] = temp_data_holder[2][int(element[5])] + 1\n",
    "            if center_X >= w/2 and center_Y < h/2:\n",
    "                if temp_data_holder[3].get(int(element[5])) == None:\n",
    "                    temp_data_holder[3][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[3][int(element[5])] = temp_data_holder[3][int(element[5])] + 1\n",
    "        for quadrant in temp_data_holder:\n",
    "            for i in range(8):\n",
    "                if quadrant.get(i) == None:\n",
    "                    temp_data_holder_2.append(0)\n",
    "                else:\n",
    "                    temp_data_holder_2.append(quadrant[i])\n",
    "        class_per_quadrant[int(temp_image_name)] = temp_data_holder_2\n",
    "    return class_per_quadrant\n",
    "    \n",
    "def freqClass():\n",
    "    data = {}\n",
    "    temp = list(raw_data.values())\n",
    "    for i in range(len(temp)):\n",
    "        per_image_data = {}\n",
    "        for element in temp[i]:\n",
    "            if per_image_data.get(int(element[5])) == None:\n",
    "                per_image_data[int(element[5])] = 1\n",
    "            else:\n",
    "                per_image_data[int(element[5])] = per_image_data[int(element[5])] + 1\n",
    "        data[int(image_names[i])] = per_image_data\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "\n",
    "SVM = False\n",
    "COLOR = False\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "\n",
    "data = {}\n",
    "\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "\n",
    "freq_quad_temp = raw()\n",
    "freq_quad = []\n",
    "quad_ratings = []\n",
    "quad_ratings_binary = []\n",
    "\n",
    "for key, value in freq_quad_temp.items():\n",
    "    if not data.get(key) == None:\n",
    "        freq_quad.append(value)\n",
    "        quad_ratings.append(data[key])\n",
    "        if data[key] >= 4:\n",
    "            quad_ratings_binary.append(1)\n",
    "        else:\n",
    "            quad_ratings_binary.append(0)\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(tf.convert_to_tensor(freq_quad))\n",
    "\n",
    "raw_linear_model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "raw_linear_model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "    loss='mean_squared_logarithmic_error',\n",
    "    metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])\n",
    "\n",
    "print(np.asarray(freq_quad).shape)\n",
    "\n",
    "history = raw_linear_model.fit(\n",
    "    tf.convert_to_tensor(freq_quad),\n",
    "    tf.convert_to_tensor(quad_ratings),\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    # Suppress logging.\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "COLOR = False\n",
    "errFiles = []\n",
    "\n",
    "def predictRaw(x, shuffle=False):\n",
    "    d = Data(directory=x,shuffle=shuffle)\n",
    "    raw_data = d.filter_confidence()\n",
    "    #print(raw_data)\n",
    "    filename_2 = [f for f in listdir(x) if isfile(join(x, f))]#[0]\n",
    "    predictions = []\n",
    "    dataholder = []\n",
    "    contr = 0\n",
    "    for filename in filename_2:\n",
    "        temp = list(raw_data[filename.replace(\".png\",\"\")])\n",
    "        rawData = []\n",
    "        for i in range(len(temp)):\n",
    "            element = [float(x) for x in list(temp[i])]\n",
    "            temp_rawData = []\n",
    "            e = []\n",
    "            #print ('Element0=%f Element1=%f Element2=%f Element3=%f Element4=%f Element5=%f'%(element[0],element[1],element[2],element[3],element[4],element[5]))\n",
    "            e.append(float((float(element[0])+float(element[2]))/2))\n",
    "            e.append(float((float(element[1])+float(element[3]))/2))\n",
    "            e.append(float(element[5]))\n",
    "            temp_rawData.extend(e)\n",
    "            rawData.extend(temp_rawData)\n",
    "        dataholder.append(rawData)\n",
    "    for key in range(len(dataholder)):\n",
    "        if len(dataholder[key]) < 237:\n",
    "            for j in range(237 - len(dataholder[key])):\n",
    "                dataholder[key].append(0)\n",
    "    try:\n",
    "        predictions = raw_linear_model.predict(tf.convert_to_tensor(dataholder))\n",
    "    except Exception as e:\n",
    "        print(\"Error in invoking raw_linear model. Exception is %s\" %(e))\n",
    "    return predictions\n",
    "    \n",
    "predicted = predictRaw(\"small_test/\")\n",
    "##print(predicted)\n",
    "data = {}\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "actual = []\n",
    "#print(data)\n",
    "i = 0\n",
    "files = [f for f in listdir(\"small_test/\") if isfile(join(\"small_test/\", f))]\n",
    "for f in files:\n",
    "    \n",
    "    if (f in errFiles):\n",
    "        continue\n",
    "    if not data.get(int(f.replace(\".png\",\"\"))) == None:\n",
    "        actual.append(data[int(f.replace(\".png\",\"\"))])\n",
    "    else:\n",
    "        actual.append(-1)\n",
    "    try:\n",
    "        print (\"FILENAME=%s PREDICATED=%.5f ACTUAL=%.5f\" %(f,predicted[i],actual[i]))\n",
    "    except Exception as e:\n",
    "        print(\":(\")\n",
    "    i+=1\n",
    "##print(actual)\n",
    "\n",
    "predicted_2 = []\n",
    "actual_2 = []\n",
    "\n",
    "print ('PREDICTED LEN IS %d and ACTUAL LEN IS %d' %(len(predicted),len(actual)))\n",
    "for i in range(len(actual)):\n",
    "    if not int(actual[i]) == -1:\n",
    "        try:\n",
    "            predicted_2.append(predicted[i])\n",
    "            actual_2.append(actual[i])\n",
    "        except Exception as e:\n",
    "            print ('something went wrong again : Exception is %s' %(e))\n",
    "\n",
    "print ('Mean Squared Error is %.20f' %(mean_squared_error(actual_2, predicted_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadrant Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "\n",
    "SVM = False\n",
    "COLOR = True\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "\n",
    "data = {}\n",
    "\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "\n",
    "if COLOR:\n",
    "    c = colorfulness()\n",
    "\n",
    "freq_quad_temp = freqQuad()\n",
    "freq_quad = []\n",
    "quad_ratings = []\n",
    "quad_ratings_binary = []\n",
    "\n",
    "for key, value in freq_quad_temp.items():\n",
    "    if not data.get(key) == None:\n",
    "        if COLOR:\n",
    "            value.append(c[key])\n",
    "        freq_quad.append(value)\n",
    "        quad_ratings.append(data[key])\n",
    "        if data[key] >= 4:\n",
    "            quad_ratings_binary.append(1)\n",
    "        else:\n",
    "            quad_ratings_binary.append(0)\n",
    "\n",
    "#print(freq_quad)\n",
    "#print(quad_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadrant linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(tf.convert_to_tensor(freq_quad))\n",
    "\n",
    "quad_linear_model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "quad_linear_model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "    loss='mean_squared_logarithmic_error',\n",
    "    metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])\n",
    "\n",
    "history = quad_linear_model.fit(\n",
    "    tf.convert_to_tensor(freq_quad),\n",
    "    tf.convert_to_tensor(quad_ratings),\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    # Suppress logging.\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadrant linear regression validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "COLOR = False\n",
    "\n",
    "def predictQuad(x,shuffle=False):\n",
    "    d = Data(directory=x,shuffle=shuffle)\n",
    "    raw_data = d.filter_confidence()\n",
    "    #print(raw_data)\n",
    "    filename_2 = [f for f in listdir(x) if isfile(join(x, f))]#[0]\n",
    "    predictions = []\n",
    "    for filename in filename_2:\n",
    "        shape = cv2.imread(x + str(filename)).shape\n",
    "        temp = list(raw_data[filename.replace(\".png\",\"\")])\n",
    "        for i in range(len(temp)):\n",
    "            element = [float(x) for x in list(temp[i])]\n",
    "            temp_data_holder = [{},{},{},{}]\n",
    "            temp_data_holder_2 = []\n",
    "            temp_image_name = image_names[i]\n",
    "            temp_image = cv2.imread(directory + temp_image_name + '.png')\n",
    "            w = temp_image.shape[0]\n",
    "            h = temp_image.shape[1]\n",
    "            center_X = (element[0]+element[2])/2\n",
    "            center_Y = (element[1]+element[3])/2\n",
    "            if center_X >= w/2 and center_Y >= h/2:\n",
    "                if temp_data_holder[0].get(int(element[5])) == None:\n",
    "                    temp_data_holder[0][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[0][int(element[5])] = temp_data_holder[0][int(element[5])] + 1\n",
    "            if center_X < w/2 and center_Y >= h/2:\n",
    "                if temp_data_holder[1].get(int(element[5])) == None:\n",
    "                    temp_data_holder[1][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[1][int(element[5])] = temp_data_holder[1][int(element[5])] + 1\n",
    "            if center_X < w/2 and center_Y < h/2:\n",
    "                if temp_data_holder[2].get(int(element[5])) == None:\n",
    "                    temp_data_holder[2][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[2][int(element[5])] = temp_data_holder[2][int(element[5])] + 1\n",
    "            if center_X >= w/2 and center_Y < h/2:\n",
    "                if temp_data_holder[3].get(int(element[5])) == None:\n",
    "                    temp_data_holder[3][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[3][int(element[5])] = temp_data_holder[3][int(element[5])] + 1\n",
    "            for quadrant in temp_data_holder:\n",
    "                for i in range(8):\n",
    "                    if quadrant.get(i) == None:\n",
    "                        temp_data_holder_2.append(0)\n",
    "                    else:\n",
    "                        temp_data_holder_2.append(quadrant[i])\n",
    "            if COLOR:\n",
    "                c = colorfulness()\n",
    "                temp_data_holder_2.append(c[int(temp_image_name)])\n",
    "        prediction = quad_linear_model.predict([temp_data_holder_2])\n",
    "        predictions.append(prediction[0][0])\n",
    "    return predictions\n",
    "    \n",
    "predicted = predictQuad(\"small_test/\")\n",
    "\n",
    "print(predicted)\n",
    "data = {}\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "actual = []\n",
    "#print(data)\n",
    "files = [f for f in listdir(\"small_test/\") if isfile(join(\"small_test/\", f))]\n",
    "for f in files:\n",
    "    if not data.get(int(f.replace(\".png\",\"\"))) == None:\n",
    "        actual.append(data[int(f.replace(\".png\",\"\"))])\n",
    "    else:\n",
    "        actual.append(-1)\n",
    "print(actual)\n",
    "\n",
    "predicted_2 = []\n",
    "actual_2 = []\n",
    "for i in range(len(actual)):\n",
    "    if not actual[i] == -1:\n",
    "        try:\n",
    "            predicted_2.append(predicted[i])\n",
    "            actual_2.append(actual[i])\n",
    "        except Exception as e:\n",
    "            print(\"something went wrong again :(\")\n",
    "\n",
    "mean_squared_error(actual_2, predicted_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Quadrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "apply_scaler = True\n",
    "\n",
    "#print(avg_pos)\n",
    "\n",
    "if apply_scaler:\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler1.fit(freq_quad)\n",
    "    quad_data = scaler1.transform(freq_quad)\n",
    "else:\n",
    "    quad_data = freq_quad\n",
    "\n",
    "quad_clf = svm.SVC()\n",
    "\n",
    "quad_clf.fit(quad_data, quad_ratings_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadrant SVM Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def predictQuadSVM(x,shuffle=False):\n",
    "    d = Data(directory=x,shuffle=shuffle)\n",
    "    raw_data = d.filter_confidence()\n",
    "    #print(raw_data)\n",
    "    filename_2 = [f for f in listdir(x) if isfile(join(x, f))]#[0]\n",
    "    predictions = []\n",
    "    for filename in filename_2:\n",
    "        shape = cv2.imread(x + str(filename)).shape\n",
    "        temp = list(raw_data[filename.replace(\".png\",\"\")])\n",
    "        for i in range(len(temp)):\n",
    "            element = temp[i]\n",
    "            temp_data_holder = [{},{},{},{}]\n",
    "            temp_data_holder_2 = []\n",
    "            temp_image_name = image_names[i]\n",
    "            temp_image = cv2.imread(directory + temp_image_name + '.png')\n",
    "            w = temp_image.shape[0]\n",
    "            h = temp_image.shape[1]\n",
    "            #print(\"Width: \" + str(w) + \" Height: \" + str(h))\n",
    "            center_X = float((element[0]+element[2])/2)\n",
    "            center_Y = float((element[1]+element[3])/2)\n",
    "            #print(\"Position: (\" + str(center_X) + \", \" + str(center_Y) + \")\")\n",
    "            print(int(element[5]))\n",
    "            if center_X >= w/2 and center_Y >= h/2:\n",
    "                print(\"Top right\")\n",
    "                if temp_data_holder[0].get(int(element[5])) == None:\n",
    "                    temp_data_holder[0][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[0][int(element[5])] = temp_data_holder[0][int(element[5])] + 1\n",
    "            if center_X < w/2 and center_Y >= h/2:\n",
    "                print(\"Top left\")\n",
    "                if temp_data_holder[1].get(int(element[5])) == None:\n",
    "                    temp_data_holder[1][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[1][int(element[5])] = temp_data_holder[1][int(element[5])] + 1\n",
    "            if center_X < w/2 and center_Y < h/2:\n",
    "                print(\"Bottom left\")\n",
    "                if temp_data_holder[2].get(int(element[5])) == None:\n",
    "                    temp_data_holder[2][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[2][int(element[5])] = temp_data_holder[2][int(element[5])] + 1\n",
    "            if center_X >= w/2 and center_Y < h/2:\n",
    "                print(\"Bottom right\")\n",
    "                if temp_data_holder[3].get(int(element[5])) == None:\n",
    "                    temp_data_holder[3][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[3][int(element[5])] = temp_data_holder[3][int(element[5])] + 1\n",
    "            #print(temp_data_holder)\n",
    "            for quadrant in temp_data_holder:\n",
    "                for i in range(8):\n",
    "                    if quadrant.get(i) == None:\n",
    "                        temp_data_holder_2.append(0)\n",
    "                    else:\n",
    "                        temp_data_holder_2.append(quadrant[i])\n",
    "            if COLOR:\n",
    "                c = colorfulness()\n",
    "                temp_data_holder_2.append(c[int(temp_image_name)])\n",
    "            try:\n",
    "                \n",
    "                #print(temp_data_holder_2)\n",
    "                prediction = quad_clf.predict([temp_data_holder_2])\n",
    "                predictions.append(prediction)\n",
    "            except Exception as e:\n",
    "                print(\"Something went wrong but we are gonna pretend like it didn't happen :)\")\n",
    "    return predictions\n",
    "\n",
    "print(predictQuadSVM(\n",
    "    #\"small_test/\"\n",
    "    \"dataset/images/english/\"\n",
    "    ))\n",
    "data = {}\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "print(data[int([f for f in listdir(\"small_test/\") if isfile(join(\"small_test/\", f))][0].replace(\".png\",\"\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadrant Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "apply_scaler3 = True\n",
    "\n",
    "if apply_scaler3:\n",
    "    scaler3 = StandardScaler()\n",
    "    scaler3.fit(freq_quad)\n",
    "    quad_data2 = scaler3.transform(freq_quad)\n",
    "else:\n",
    "    quad_data2 = freq_quad\n",
    "# Instantiate model with 1000 decision trees\n",
    "quad_rf = RandomForestRegressor(n_estimators = 9999, random_state = 42)\n",
    "# Train the model on training data\n",
    "quad_rf.fit(quad_data2, quad_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadrant Random Forest Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def predictQuadRF(x,shuffle=False):\n",
    "    d = Data(directory=x,shuffle=shuffle)\n",
    "    raw_data = d.filter_confidence()\n",
    "    #print(raw_data)\n",
    "    filename_2 = [f for f in listdir(x) if isfile(join(x, f))]#[0]\n",
    "    predictions = []\n",
    "    for filename in filename_2:\n",
    "        shape = cv2.imread(x + str(filename)).shape\n",
    "        temp = list(raw_data[filename.replace(\".png\",\"\")])\n",
    "        for i in range(len(temp)):\n",
    "            element = temp[i]\n",
    "            temp_data_holder = [{},{},{},{}]\n",
    "            temp_data_holder_2 = []\n",
    "            temp_image_name = image_names[i]\n",
    "            temp_image = cv2.imread(directory + temp_image_name + '.png')\n",
    "            w = temp_image.shape[0]\n",
    "            h = temp_image.shape[1]\n",
    "            #print(temp)\n",
    "            center_X = (int(element[0])+int(element[2]))/2\n",
    "            center_Y = (int(element[1])+int(element[3]))/2\n",
    "            if center_X >= w/2 and center_Y >= h/2:\n",
    "                if temp_data_holder[0].get(int(element[5])) == None:\n",
    "                    temp_data_holder[0][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[0][int(element[5])] = temp_data_holder[0][int(element[5])] + 1\n",
    "            if center_X < w/2 and center_Y >= h/2:\n",
    "                if temp_data_holder[1].get(int(element[5])) == None:\n",
    "                    temp_data_holder[1][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[1][int(element[5])] = temp_data_holder[1][int(element[5])] + 1\n",
    "            if center_X < w/2 and center_Y < h/2:\n",
    "                if temp_data_holder[2].get(int(element[5])) == None:\n",
    "                    temp_data_holder[2][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[2][int(element[5])] = temp_data_holder[2][int(element[5])] + 1\n",
    "            if center_X >= w/2 and center_Y < h/2:\n",
    "                if temp_data_holder[3].get(int(element[5])) == None:\n",
    "                    temp_data_holder[3][int(element[5])] = 1\n",
    "                else:\n",
    "                    temp_data_holder[3][int(element[5])] = temp_data_holder[3][int(element[5])] + 1\n",
    "        for quadrant in temp_data_holder:\n",
    "            for i in range(8):\n",
    "                if quadrant.get(i) == None:\n",
    "                    temp_data_holder_2.append(0)\n",
    "                else:\n",
    "                    temp_data_holder_2.append(quadrant[i])\n",
    "        if COLOR:\n",
    "            c = colorfulness()\n",
    "            temp_data_holder_2.append(c[int(temp_image_name)])\n",
    "        try:\n",
    "            prediction = quad_rf.predict([temp_data_holder_2])\n",
    "            predictions.append(prediction[0])\n",
    "        except Exception as e:\n",
    "            print(\"Something went wrong hehe :)\")\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "predicted = predictQuadRF(\"small_test/\")\n",
    "\n",
    "print(predicted)\n",
    "data = {}\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "actual = []\n",
    "#print(data)\n",
    "files = [f for f in listdir(\"small_test/\") if isfile(join(\"small_test/\", f))]\n",
    "for f in files:\n",
    "    if not data.get(int(f.replace(\".png\",\"\"))) == None:\n",
    "        actual.append(data[int(f.replace(\".png\",\"\"))])\n",
    "    else:\n",
    "        actual.append(-1)\n",
    "print(actual)\n",
    "\n",
    "predicted_2 = []\n",
    "actual_2 = []\n",
    "for i in range(len(actual)):\n",
    "    if not actual[i] == -1:\n",
    "        try:\n",
    "            predicted_2.append(predicted[i])\n",
    "            actual_2.append(actual[i])\n",
    "        except Exception as e:\n",
    "            print(\"something went wrong again :(\")\n",
    "\n",
    "mean_squared_error(actual_2, predicted_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Position Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "\n",
    "SVM = True\n",
    "COLOR = False\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "\n",
    "data = {}\n",
    "\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "\n",
    "avg_pos_temp = avgPos()\n",
    "avg_pos = []\n",
    "ratings = []\n",
    "if COLOR:\n",
    "    c = colorfulness()\n",
    "\n",
    "for key, element in avg_pos_temp.items():\n",
    "    if not data.get(key) == None:\n",
    "        temp_data = []\n",
    "        shape = cv2.imread(directory + str(key) + \".png\").shape\n",
    "        #print(shape)\n",
    "        for i in range(8):\n",
    "            if element.get(i) == None:\n",
    "                temp_data.append(0)\n",
    "                temp_data.append(0)\n",
    "            else:\n",
    "                temp_data.append(element[i][0]\n",
    "                #/shape[1]\n",
    "                )\n",
    "                temp_data.append(element[i][1]\n",
    "                #/shape[0]\n",
    "                )\n",
    "        if COLOR:\n",
    "            temp_data.append(c[int(key)])\n",
    "        if SVM:\n",
    "            avg_pos.append(temp_data)\n",
    "            if data[key] >= 4.5:\n",
    "                ratings.append(1)\n",
    "            else:\n",
    "                ratings.append(0)\n",
    "        else:\n",
    "            avg_pos.append(np.asarray(temp_data).astype(np.float32))\n",
    "            ratings.append((data[key]))\n",
    "\n",
    "#print(avg_pos)\n",
    "#print(ratings)\n",
    "\n",
    "if not SVM:\n",
    "    normalizer.adapt(tf.convert_to_tensor(avg_pos))\n",
    "\n",
    "    linear_model = tf.keras.Sequential([\n",
    "        normalizer,\n",
    "        layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    linear_model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "        loss='mean_squared_logarithmic_error',\n",
    "        metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])\n",
    "\n",
    "    history = linear_model.fit(\n",
    "        tf.convert_to_tensor(avg_pos),\n",
    "        tf.convert_to_tensor(ratings),\n",
    "        epochs=100,\n",
    "        validation_split=0.2,\n",
    "        # Suppress logging.\n",
    "        verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "apply_scaler = True\n",
    "\n",
    "#print(avg_pos)\n",
    "\n",
    "if apply_scaler:\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler1.fit(avg_pos)\n",
    "    data = scaler1.transform(avg_pos)\n",
    "else:\n",
    "    data = avg_pos\n",
    "\n",
    "clf = svm.SVC()\n",
    "\n",
    "clf.fit(data, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "COLOR = False\n",
    "\n",
    "def predictSVM(x):\n",
    "    if COLOR:\n",
    "        c = colorfulness()\n",
    "    d = Data(directory=x)\n",
    "    raw_data = d.filter_confidence()\n",
    "    #print(raw_data)\n",
    "    filename_2 = [f for f in listdir(x) if isfile(join(x, f))]#[0]\n",
    "    predictions = []\n",
    "    for filename in filename_2:\n",
    "        shape = cv2.imread(x + str(filename)).shape\n",
    "        temp = list(raw_data[filename.replace(\".png\",\"\")])\n",
    "        for i in range(len(temp)):\n",
    "            element = temp[i]\n",
    "            avg_pixel_position_for_class = {}\n",
    "            count = {}\n",
    "            if avg_pixel_position_for_class.get(int(element[5])) == None:\n",
    "                avg_pixel_position_for_class[int(element[5])] = ((float(element[0])+float(element[2]))/2,(float(element[1])+float(element[3]))/2)\n",
    "                count[int(element[5])] = 1\n",
    "            else:\n",
    "                count[int(element[5])] += 1\n",
    "                avg_pixel_position_for_class[int(element[5])] = ((((float(element[0])+float(element[2]))/2)+avg_pixel_position_for_class[int(element[5])][0])/count[int(element[5])],(((float(element[1])+float(element[3]))/2)+avg_pixel_position_for_class[int(element[5])][0])/count[int(element[5])])\n",
    "        processed_data = []\n",
    "        for i in range(8):\n",
    "            if avg_pixel_position_for_class.get(i) == None:\n",
    "                processed_data.append(0)\n",
    "                processed_data.append(0)\n",
    "            else:\n",
    "                processed_data.append(avg_pixel_position_for_class[i][0]\n",
    "                #/shape[1]\n",
    "                )\n",
    "                processed_data.append(avg_pixel_position_for_class[i][1]\n",
    "                #/shape[0]\n",
    "                )\n",
    "        if COLOR:\n",
    "            processed_data.append(c[int(filename.replace(\".png\",\"\"))])\n",
    "\n",
    "        prediction = clf.predict([processed_data])\n",
    "        predictions.append(prediction[0])\n",
    "    return predictions\n",
    "\n",
    "predicted = predictSVM(\"small_test/\")\n",
    "\n",
    "#print(predicted)\n",
    "data = {}\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "actual = []\n",
    "#print(data)\n",
    "files = [f for f in listdir(\"small_test/\") if isfile(join(\"small_test/\", f))]\n",
    "for f in files:\n",
    "    if not data.get(int(f.replace(\".png\",\"\"))) == None:\n",
    "        act_rating = data[int(f.replace(\".png\",\"\"))]\n",
    "        if act_rating >= 4:\n",
    "            actual.append(1)\n",
    "        else:\n",
    "            actual.append(0)\n",
    "    else:\n",
    "        actual.append(-1)\n",
    "#print(actual)\n",
    "\n",
    "predicted_2 = []\n",
    "actual_2 = []\n",
    "for i in range(len(actual)):\n",
    "    if not actual[i] == -1:\n",
    "        try:\n",
    "            predicted_2.append(predicted[i])\n",
    "            actual_2.append(actual[i])\n",
    "        except Exception as e:\n",
    "            print(\"something went wrong again :(\")\n",
    "\n",
    "print(predicted_2)\n",
    "print(actual_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate Average Position Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def predict(x):\n",
    "    d = Data(directory=x)\n",
    "    raw_data = d.filter_confidence()\n",
    "    #print(raw_data)\n",
    "    filename_2 = [f for f in listdir(x) if isfile(join(x, f))]#[0]\n",
    "    predictions = []\n",
    "    for filename in filename_2:\n",
    "        temp_image_name = image_names[i]\n",
    "        shape = cv2.imread(x + str(filename)).shape\n",
    "        temp = list(raw_data[filename.replace(\".png\",\"\")])\n",
    "        for i in range(len(temp)):\n",
    "            element = [float(x) for x in list(temp[i])]\n",
    "            avg_pixel_position_for_class = {}\n",
    "            #print(avg_pixel_position_for_class.get(int(element[5])))\n",
    "            count = {}\n",
    "            if avg_pixel_position_for_class.get(int(element[5])) == None:\n",
    "                avg_pixel_position_for_class[int(element[5])] = ((float(element[0])+float(element[2]))/2,(float(element[1])+float(element[3]))/2)\n",
    "                count[int(element[5])] = 1\n",
    "            else:\n",
    "                count[int(element[5])] += 1\n",
    "                avg_pixel_position_for_class[int(element[5])] = ((((float(element[0])+float(element[2]))/2)+avg_pixel_position_for_class[int(element[5])][0])/count[int(element[5])],(((float(element[1])+float(element[3]))/2)+avg_pixel_position_for_class[int(element[5])][0])/count[int(element[5])])\n",
    "            processed_data = []\n",
    "            for i in range(8):\n",
    "                if avg_pixel_position_for_class.get(i) == None:\n",
    "                    processed_data.append(0)\n",
    "                    processed_data.append(0)\n",
    "                else:\n",
    "                    processed_data.append(avg_pixel_position_for_class[i][0]\n",
    "                    #/shape[1]\n",
    "                    )\n",
    "                    processed_data.append(avg_pixel_position_for_class[i][1]\n",
    "                    #/shape[0]\n",
    "                    )\n",
    "            #print(processed_data)\n",
    "            if COLOR:\n",
    "                c = colorfulness()\n",
    "                processed_data.append(c[int(temp_image_name)])\n",
    "            prediction = linear_model.predict([processed_data])\n",
    "            predictions.append(prediction[0][0])\n",
    "    return predictions\n",
    "    \n",
    "predicted = predict(\"small_test/\")\n",
    "actual = []\n",
    "#print(data)\n",
    "files = [f for f in listdir(\"small_test/\") if isfile(join(\"small_test/\", f))]\n",
    "for f in files:\n",
    "    if not data.get(int(f.replace(\".png\",\"\"))) == None:\n",
    "        actual.append(data[int(f.replace(\".png\",\"\"))])\n",
    "    else:\n",
    "        actual.append(-1)\n",
    "#print(actual)\n",
    "\n",
    "predicted_2 = []\n",
    "actual_2 = []\n",
    "for i in range(len(actual)):\n",
    "    if not actual[i] == -1:\n",
    "        try:\n",
    "            predicted_2.append(predicted[i])\n",
    "            actual_2.append(actual[i])\n",
    "        except Exception as e:\n",
    "            print(\"something went wrong again :(\")\n",
    "\n",
    "mean_squared_error(actual_2, predicted_2)\n",
    "#[    0.77215     0.69101     0.83729      0.1006     0.12892    0.044708           0           0           0           0           0           0, 0.73748, 0.036506, 0.45679, 0.69613]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "\n",
    "SVM = False\n",
    "RANDOMFOREST = True\n",
    "\n",
    "data = {}\n",
    "\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "\n",
    "class_freq_temp = freqClass()\n",
    "\n",
    "#print(class_freq_temp)\n",
    "\n",
    "data_2 = {}\n",
    "\n",
    "class_freq = {}\n",
    "\n",
    "for key, value in class_freq_temp.items():\n",
    "    if not data.get(key) == None:\n",
    "        temp_val = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[]}\n",
    "        for i in range(8):\n",
    "            if value.get(i) == None:\n",
    "                temp_val[i] = 0\n",
    "            else:\n",
    "                temp_val[i] = value[i]\n",
    "        class_freq[key] = temp_val\n",
    "\n",
    "for key in list(class_freq_temp.keys()):\n",
    "    if not data.get(key) == None:\n",
    "        data_2[key] = data[key]\n",
    "\n",
    "train_features = list(class_freq.copy().values())\n",
    "train_labels = list(data_2.copy().values())\n",
    "\n",
    "if not SVM:\n",
    "    if not RANDOMFOREST:\n",
    "        for i in range(len(train_features)):\n",
    "            train_features[i] = np.asarray(list(train_features[i].values())).astype(np.float32)\n",
    "\n",
    "        train_features = tf.convert_to_tensor(train_features)\n",
    "        train_labels = tf.convert_to_tensor(np.asarray(train_labels).astype(np.float32))\n",
    "    else:\n",
    "        for i in range(len(train_features)):\n",
    "            train_features[i] = list(train_features[i].values())\n",
    "else:\n",
    "    for i in range(len(train_features)):\n",
    "        train_features[i] = list(train_features[i].values())\n",
    "    for i in range(len(train_labels)):\n",
    "        if train_labels[i]>=4:\n",
    "            train_labels[i]=1\n",
    "        else:\n",
    "            train_labels[i]=0\n",
    "\n",
    "#print(train_features)\n",
    "#print(train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "apply_scaler3 = True\n",
    "\n",
    "if apply_scaler3:\n",
    "    scaler3 = StandardScaler()\n",
    "    scaler3.fit(train_features)\n",
    "    data3 = scaler3.transform(train_features)\n",
    "else:\n",
    "    data3 = train_features\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(data3, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def predictRandomForest2(x):\n",
    "    d = Data(directory=x)\n",
    "    raw_data = d.filter_confidence()\n",
    "    #print(raw_data)\n",
    "    filename_2 = [f for f in listdir(x) if isfile(join(x, f))]#[0]\n",
    "    predictions = []\n",
    "    for filename in filename_2:\n",
    "        shape = cv2.imread(x + str(filename)).shape\n",
    "        temp = list(raw_data[filename.replace(\".png\",\"\")])\n",
    "        formatted_data = []\n",
    "        for i in range(len(temp)):\n",
    "            element = temp[i]\n",
    "            per_image_data = {}\n",
    "            if per_image_data.get(int(element[5])) == None:\n",
    "                per_image_data[int(element[5])] = 1\n",
    "            else:\n",
    "                per_image_data[int(element[5])] = per_image_data[int(element[5])] + 1\n",
    "        for i in range(8):\n",
    "            if per_image_data.get(i) == None:\n",
    "                formatted_data.append(0)\n",
    "            else:\n",
    "                formatted_data.append(per_image_data[i])\n",
    "        predictions.append(float(rf.predict([formatted_data])[0]))\n",
    "    return predictions\n",
    "\n",
    "predicted = predictRandomForest2(\"small_test/\")\n",
    "actual = []\n",
    "#print(data)\n",
    "files = [f for f in listdir(\"small_test/\") if isfile(join(\"small_test/\", f))]\n",
    "for f in files:\n",
    "    if not data.get(int(f.replace(\".png\",\"\"))) == None:\n",
    "        actual.append(data[int(f.replace(\".png\",\"\"))])\n",
    "    else:\n",
    "        actual.append(-1)\n",
    "#print(actual)\n",
    "\n",
    "predicted_2 = []\n",
    "actual_2 = []\n",
    "for i in range(len(actual)):\n",
    "    if not actual[i] == -1:\n",
    "        try:\n",
    "            predicted_2.append(predicted[i])\n",
    "            actual_2.append(actual[i])\n",
    "        except Exception as e:\n",
    "            print(\"something went wrong again :(\")\n",
    "\n",
    "mean_squared_error(actual_2, predicted_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "apply_scaler2 = True\n",
    "\n",
    "if apply_scaler2:\n",
    "    scaler2 = StandardScaler()\n",
    "    scaler2.fit(train_features)\n",
    "    data2 = scaler2.transform(train_features)\n",
    "else:\n",
    "    data2 = train_features\n",
    "\n",
    "clf = svm.SVC()\n",
    "\n",
    "clf.fit(data2, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def predictSVM2(x):\n",
    "    d = Data(directory=x)\n",
    "    filename = [f for f in listdir(x) if isfile(join(x, f))][0]\n",
    "    raw_data = d.filter_confidence()\n",
    "    temp = list(raw_data.values())\n",
    "    shape = cv2.imread(x + str(filename)).shape\n",
    "    formatted_data = []\n",
    "    for i in range(len(temp)):\n",
    "        per_image_data = {}\n",
    "        for element in temp[i]:\n",
    "            if per_image_data.get(int(element[5])) == None:\n",
    "                per_image_data[int(element[5])] = 1\n",
    "            else:\n",
    "                per_image_data[int(element[5])] = per_image_data[int(element[5])] + 1\n",
    "        for i in range(8):\n",
    "            if per_image_data.get(i) == None:\n",
    "                formatted_data.append(0)\n",
    "            else:\n",
    "                formatted_data.append(per_image_data[i])\n",
    "    return float(clf.predict([formatted_data])[0])\n",
    "\n",
    "\n",
    "print(predictSVM2(\"small_test/\"))\n",
    "data = {}\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "print(data[int([f for f in listdir(\"small_test/\") if isfile(join(\"small_test/\", f))][0].replace(\".png\",\"\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.keras.Sequential([\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "linear_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mean_squared_logarithmic_error',\n",
    "    metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "print(len(list(train_features)))\n",
    "print(len(train_labels))\n",
    "\n",
    "history = linear_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    # Suppress logging.\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"small_test/\"\n",
    "\n",
    "def getPrediction(imgLink):\n",
    "    d = Data(directory=imgLink)\n",
    "    raw_data = d.filter_confidence()\n",
    "    temp = list(raw_data.values())\n",
    "    for i in range(len(temp)):\n",
    "        per_image_data = {}\n",
    "        for element in temp[i]:\n",
    "            if per_image_data.get(int(element[5])) == None:\n",
    "                per_image_data[int(element[5])] = 1\n",
    "            else:\n",
    "                per_image_data[int(element[5])] = per_image_data[int(element[5])] + 1\n",
    "    processed_data = []\n",
    "    for i in range(8):\n",
    "        if per_image_data.get(i) == None:\n",
    "            processed_data.append(0)\n",
    "        else:\n",
    "            processed_data.append(per_image_data[i])\n",
    "    prediction = linear_model.predict([processed_data])\n",
    "    return prediction\n",
    "\n",
    "    \n",
    "print(getPrediction(img_path))\n",
    "data = {}\n",
    "path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "with open(path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if \"english_resized\" in row[0]:\n",
    "            key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "            value = float(row[1])\n",
    "            data[key] = value\n",
    "print(data[int([f for f in listdir(\"small_test/\") if isfile(join(\"small_test/\", f))][0].replace(\".png\",\"\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't use raw\n",
    "# do all combinations of average position, class frequency, and quadrants\n",
    "# create table of results\n",
    "# create narrative surrounding results\n",
    "# base it off of related works and other papers\n",
    "\n",
    "def mixFeatures(avgPosition,quadrants,classfreq,raw,modeltype, c, use_independent_data=False, shuffle=False):\n",
    "    directory = \"dataset/images/english/\"\n",
    "    if use_independent_data:\n",
    "\n",
    "        data = Data(\n",
    "            directory=directory,\n",
    "            shuffle=shuffle\n",
    "            )\n",
    "\n",
    "        raw_data = data.filter_confidence()\n",
    "        image_names = list(raw_data.keys())\n",
    "        image_path = data.images()\n",
    "        images = {}\n",
    "\n",
    "        for i in range(len(image_path)):\n",
    "            images[image_names[i]] = cv2.imread(image_path[i])\n",
    "    import csv\n",
    "    path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "    data = {}\n",
    "    with open(path) as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if \"english_resized\" in row[0]:\n",
    "                key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "                value = float(row[1])\n",
    "                data[key] = value\n",
    "    training = {}\n",
    "    a = []\n",
    "    b = []\n",
    "    def createSVM(training, labels, apply_scaler2=True):\n",
    "        from sklearn import svm\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "        if apply_scaler2:\n",
    "            scaler2 = StandardScaler()\n",
    "            scaler2.fit(training)\n",
    "            data2 = scaler2.transform(training)\n",
    "        else:\n",
    "            data2 = training\n",
    "\n",
    "        clf = svm.SVC()\n",
    "\n",
    "        clf.fit(data2, labels)\n",
    "        return clf\n",
    "    def createLin(training, labels, normalize=True):\n",
    "        if normalize:\n",
    "            normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "            normalizer.adapt(tf.convert_to_tensor(training))\n",
    "            linear_model = tf.keras.Sequential([\n",
    "                normalizer,\n",
    "                layers.Dense(units=1)\n",
    "            ])\n",
    "        else:\n",
    "            linear_model = tf.keras.Sequential([\n",
    "                layers.Dense(units=1)\n",
    "            ])\n",
    "\n",
    "        linear_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "            loss='mean_squared_logarithmic_error',\n",
    "            metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "        history = linear_model.fit(\n",
    "            tf.convert_to_tensor(training),\n",
    "            tf.convert_to_tensor(labels),\n",
    "            epochs=100,\n",
    "            validation_split=0.2,\n",
    "            # Suppress logging.\n",
    "            verbose=1)\n",
    "        return linear_model\n",
    "    def createRF(training, labels, estim=1000, r_state=42, apply_scaler3=True):\n",
    "        # Import the model we are using\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "        if apply_scaler3:\n",
    "            scaler3 = StandardScaler()\n",
    "            scaler3.fit(training)\n",
    "            data3 = scaler3.transform(training)\n",
    "        else:\n",
    "            data3 = training\n",
    "        # Instantiate model with 1000 decision trees\n",
    "        rf = RandomForestRegressor(n_estimators = estim, random_state = r_state)\n",
    "        # Train the model on training data\n",
    "        rf.fit(data3, labels)\n",
    "        return rf\n",
    "    def formatAvgPos(SVM=False,COLOR=False):\n",
    "        import numpy as np\n",
    "\n",
    "        avg_pos_temp = avgPos()\n",
    "        avg_pos = {}\n",
    "        ratings = []\n",
    "        if COLOR:\n",
    "            c = colorfulness()\n",
    "\n",
    "        for key, element in avg_pos_temp.items():\n",
    "            if not data.get(key) == None:\n",
    "                temp_data = []\n",
    "                shape = cv2.imread(directory + str(key) + \".png\").shape\n",
    "                #print(shape)\n",
    "                for i in range(8):\n",
    "                    if element.get(i) == None:\n",
    "                        temp_data.append(0)\n",
    "                        temp_data.append(0)\n",
    "                    else:\n",
    "                        temp_data.append(element[i][0]\n",
    "                        #/shape[1]\n",
    "                        )\n",
    "                        temp_data.append(element[i][1]\n",
    "                        #/shape[0]\n",
    "                        )\n",
    "                if COLOR:\n",
    "                    temp_data.append(c[int(key)])\n",
    "                if SVM:\n",
    "                    avg_pos[int(key)] = temp_data\n",
    "                    if data[key] >= 4.5:\n",
    "                        ratings.append(1)\n",
    "                    else:\n",
    "                        ratings.append(0)\n",
    "                else:\n",
    "                    avg_pos[int(key)] = temp_data\n",
    "                    ratings.append((data[key]))\n",
    "        return avg_pos,ratings\n",
    "    def formatFreq(SVM=False,RANDOMFOREST=False):\n",
    "        import numpy as np\n",
    "\n",
    "        class_freq_temp = freqClass()\n",
    "\n",
    "        #print(class_freq_temp)\n",
    "\n",
    "        data_2 = {}\n",
    "\n",
    "        class_freq = {}\n",
    "\n",
    "        for key, value in class_freq_temp.items():\n",
    "            if not data.get(key) == None:\n",
    "                temp_val = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[]}\n",
    "                for i in range(8):\n",
    "                    if value.get(i) == None:\n",
    "                        temp_val[i] = 0\n",
    "                    else:\n",
    "                        temp_val[i] = value[i]\n",
    "                class_freq[key] = temp_val\n",
    "\n",
    "        for key in list(class_freq_temp.keys()):\n",
    "            if not data.get(key) == None:\n",
    "                data_2[key] = data[key]\n",
    "\n",
    "        return class_freq, data_2\n",
    "    def formatRaw(SVM=False, COLOR=False):\n",
    "        import numpy as np\n",
    "\n",
    "        def raw():\n",
    "            rawData = {}\n",
    "            temp = list(raw_data.values())\n",
    "            for i in range(len(temp)):\n",
    "                temp_rawData = []\n",
    "                for element in temp[i]:\n",
    "                    e = []\n",
    "                    e.append(float((float(element[0])+float(element[2]))/2))\n",
    "                    e.append(float((float(element[1])+float(element[3]))/2))\n",
    "                    e.append(float(element[5]))\n",
    "                    temp_rawData.extend(e)\n",
    "                rawData[int(image_names[i])] = temp_rawData\n",
    "            sizes = []\n",
    "            for i in range(len(rawData)):\n",
    "                sizes.append(len(rawData[i]))\n",
    "            max_size = max(sizes)\n",
    "            for i in range(len(rawData)):\n",
    "                if len(rawData[i]) < max_size:\n",
    "                    for j in range(max_size - len(rawData[i])):\n",
    "                        rawData[i].append(0)\n",
    "            return rawData\n",
    "\n",
    "        freq_quad_temp = raw()\n",
    "        freq_quad = []\n",
    "        quad_ratings = []\n",
    "        quad_ratings_binary = []\n",
    "\n",
    "        for key, value in freq_quad_temp.items():\n",
    "            if not data.get(key) == None:\n",
    "                freq_quad.append(value)\n",
    "                quad_ratings.append(data[key])\n",
    "                if data[key] >= 4:\n",
    "                    quad_ratings_binary.append(1)\n",
    "                else:\n",
    "                    quad_ratings_binary.append(0)\n",
    "        if SVM:\n",
    "            return freq_quad_temp, quad_ratings_binary\n",
    "        else:\n",
    "            return freq_quad_temp, quad_ratings\n",
    "    def formatQuad(SVM=False, COLOR=False):\n",
    "        import numpy as np\n",
    "\n",
    "        if COLOR:\n",
    "            c = colorfulness()\n",
    "\n",
    "        freq_quad_temp = freqQuad()\n",
    "        freq_quad = {}\n",
    "        quad_ratings = []\n",
    "        quad_ratings_binary = []\n",
    "\n",
    "        for key, value in freq_quad_temp.items():\n",
    "            if not data.get(key) == None:\n",
    "                if COLOR:\n",
    "                    value.append(c[key])\n",
    "                freq_quad[key] = value\n",
    "                quad_ratings.append(data[key])\n",
    "                if data[key] >= 4:\n",
    "                    quad_ratings_binary.append(1)\n",
    "                else:\n",
    "                    quad_ratings_binary.append(0)\n",
    "        if SVM:\n",
    "            return freq_quad, quad_ratings_binary\n",
    "        else:\n",
    "            return freq_quad, quad_ratings\n",
    "    def integrateData(input):\n",
    "        for key, value in input.items():\n",
    "            if training.get(key) == None:\n",
    "                training[key] = value\n",
    "            else:\n",
    "                training[key].extend(value)\n",
    "    def makeTrainTest(SVM=False, threshold=4):\n",
    "        for key, value in training.items():\n",
    "            if not data.get(key) == None:\n",
    "                a.append(value)\n",
    "                if SVM:\n",
    "                    data_index = data[key]\n",
    "                    if data_index > threshold:\n",
    "                        b.append(1)\n",
    "                    else:\n",
    "                        b.append(0)\n",
    "                else:\n",
    "                    b.append(data[key])\n",
    "    s = None\n",
    "    r = None\n",
    "    model = None\n",
    "    if modeltype == \"SVM\":\n",
    "        s = True\n",
    "    else:\n",
    "        s = False\n",
    "    if modeltype == \"RF\":\n",
    "        r = True\n",
    "    else:\n",
    "        r = False\n",
    "    if avgPosition:\n",
    "        integrateData(formatAvgPos(SVM=s,COLOR=c)[0])\n",
    "    if quadrants:\n",
    "        integrateData(formatQuad(SVM=s,COLOR=c)[0])\n",
    "    if classfreq:\n",
    "        integrateData(formatFreq(SVM=s,RANDOMFOREST=r)[0])\n",
    "    if raw:\n",
    "        integrateData(formatRaw(SVM=s,COLOR=c)[0])\n",
    "    makeTrainTest(SVM=s)\n",
    "    if r:\n",
    "        model = createRF(a,b)\n",
    "    if s:\n",
    "        model = createSVM(a,b)\n",
    "    else:\n",
    "        model = createLin(a,b)\n",
    "    return model\n",
    "\n",
    "m = mixFeatures(True,False,True,False,\"SVM\",False)\n",
    "\n",
    "m.predict([[0,1,0,2,3,1,3,5,1,3,7,3,1,8,2,6,3,0,0,0,0,0,0,0]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix Features Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\woprg/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-9-27 Python-3.10.4 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7031701 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\woprg/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-9-27 Python-3.10.4 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7031701 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1],\n",
       " 0.7647058823529411)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def mixValid(x,avgPosition,quadrants,classfreq,raw,modeltype, c,shuffle=False):\n",
    "    d = Data(directory=x,shuffle=shuffle)\n",
    "    raw_data = d.filter_confidence()\n",
    "    image_names = list(raw_data.keys())\n",
    "    image_path = d.images()\n",
    "    images = {}\n",
    "\n",
    "\n",
    "    for i in range(len(image_path)):\n",
    "        images[image_names[i]] = cv2.imread(image_path[i])\n",
    "    #print(raw_data)\n",
    "    import csv\n",
    "    path = r\"dataset\\preprocess\\train_means_list.csv\"\n",
    "    data = {}\n",
    "    with open(path) as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if \"english_resized\" in row[0]:\n",
    "                key = int(row[0][row[0].rfind(\"/\")+1:row[0].rfind(\".\")])\n",
    "                value = float(row[1])\n",
    "                data[key] = value\n",
    "    testing = {}\n",
    "    a = []\n",
    "    b = []\n",
    "    def formatAvgPos(data_2=None, SVM=False):\n",
    "        import numpy as np\n",
    "\n",
    "        if data_2 == None:\n",
    "            avg_pos_temp = avgPos()\n",
    "        else:\n",
    "            def avgPosOfGiven(d):\n",
    "                temp = list(d.values())\n",
    "                avg_element_position = {}\n",
    "                for i in range(len(temp)):\n",
    "                    avg_pixel_position_for_class = {}\n",
    "                    count = {}\n",
    "                    for element in temp[i]:\n",
    "                        if avg_pixel_position_for_class.get(int(element[5])) == None:\n",
    "                            avg_pixel_position_for_class[int(element[5])] = ((float(element[0])+float(element[2]))/2,(float(element[1])+float(element[3]))/2)\n",
    "                            count[int(element[5])] = 1\n",
    "                        else:\n",
    "                            count[int(element[5])] += 1\n",
    "                            avg_pixel_position_for_class[int(element[5])] = ((((float(element[0])+float(element[2]))/2)+avg_pixel_position_for_class[int(element[5])][0])/count[int(element[5])],(((float(element[1])+float(element[3]))/2)+avg_pixel_position_for_class[int(element[5])][0])/count[int(element[5])])\n",
    "                            \n",
    "                    avg_element_position[int(image_names[i])] = avg_pixel_position_for_class\n",
    "                    #print(count)\n",
    "\n",
    "                return avg_element_position\n",
    "            avg_pos_temp = avgPosOfGiven(raw_data)\n",
    "        avg_pos = {}\n",
    "        ratings = []\n",
    "\n",
    "        for key, element in avg_pos_temp.items():\n",
    "            if not data.get(key) == None:\n",
    "                temp_data = []\n",
    "                shape = cv2.imread(directory + str(key) + \".png\").shape\n",
    "                #print(shape)\n",
    "                for i in range(8):\n",
    "                    if element.get(i) == None:\n",
    "                        temp_data.append(0)\n",
    "                        temp_data.append(0)\n",
    "                    else:\n",
    "                        temp_data.append(element[i][0]\n",
    "                        #/shape[1]\n",
    "                        )\n",
    "                        temp_data.append(element[i][1]\n",
    "                        #/shape[0]\n",
    "                        )\n",
    "                if SVM:\n",
    "                    avg_pos[int(key)] = temp_data\n",
    "                    if data[key] >= 4.5:\n",
    "                        ratings.append(1)\n",
    "                    else:\n",
    "                        ratings.append(0)\n",
    "                else:\n",
    "                    avg_pos[int(key)] = temp_data\n",
    "                    ratings.append((data[key]))\n",
    "        return avg_pos,ratings\n",
    "    \n",
    "    def formatFreq(data_2=None, SVM=False,RANDOMFOREST=False):\n",
    "        import numpy as np\n",
    "\n",
    "        if data_2==None:\n",
    "            class_freq_temp = freqClass()\n",
    "        else:\n",
    "            def freqClass(d):\n",
    "                data = {}\n",
    "                temp = list(d.values())\n",
    "                for i in range(len(temp)):\n",
    "                    per_image_data = {}\n",
    "                    for element in temp[i]:\n",
    "                        if per_image_data.get(int(element[5])) == None:\n",
    "                            per_image_data[int(element[5])] = 1\n",
    "                        else:\n",
    "                            per_image_data[int(element[5])] = per_image_data[int(element[5])] + 1\n",
    "                    data[int(image_names[i])] = per_image_data\n",
    "                return data\n",
    "            class_freq_temp = freqClass(raw_data)\n",
    "        #print(class_freq_temp)\n",
    "\n",
    "        data_3 = {}\n",
    "\n",
    "        class_freq = {}\n",
    "\n",
    "        for key, value in class_freq_temp.items():\n",
    "            if not data.get(key) == None:\n",
    "                temp_val = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[]}\n",
    "                for i in range(8):\n",
    "                    if value.get(i) == None:\n",
    "                        temp_val[i] = 0\n",
    "                    else:\n",
    "                        temp_val[i] = value[i]\n",
    "                class_freq[key] = list(temp_val.values())\n",
    "\n",
    "        for key in list(class_freq_temp.keys()):\n",
    "            if not data.get(key) == None:\n",
    "                data_3[key] = data[key]\n",
    "\n",
    "        return class_freq, data_3\n",
    "    def formatRaw(data_2=None, SVM=False):\n",
    "        import numpy as np\n",
    "\n",
    "        if data_2 == None:\n",
    "            freq_quad_temp = raw()\n",
    "        else:\n",
    "            def raw(d):\n",
    "                rawData = {}\n",
    "                temp = list(d.values())\n",
    "                for i in range(len(temp)):\n",
    "                    temp_rawData = []\n",
    "                    for element in temp[i]:\n",
    "                        e = []\n",
    "                        e.append(float((float(element[0])+float(element[2]))/2))\n",
    "                        e.append(float((float(element[1])+float(element[3]))/2))\n",
    "                        e.append(float(element[5]))\n",
    "                        temp_rawData.extend(e)\n",
    "                    rawData[int(image_names[i])] = temp_rawData\n",
    "                sizes = []\n",
    "                for i in range(len(rawData)):\n",
    "                    sizes.append(len(rawData[i]))\n",
    "                max_size = max(sizes)\n",
    "                for i in range(len(rawData)):\n",
    "                    if len(rawData[i]) < max_size:\n",
    "                        for j in range(max_size - len(rawData[i])):\n",
    "                            rawData[i].append(0)\n",
    "                return rawData\n",
    "            freq_quad_temp=raw(raw_data)\n",
    "        freq_quad = []\n",
    "        quad_ratings = []\n",
    "        quad_ratings_binary = []\n",
    "\n",
    "        for key, value in freq_quad_temp.items():\n",
    "            if not data.get(key) == None:\n",
    "                freq_quad.append(value)\n",
    "                quad_ratings.append(data[key])\n",
    "                if data[key] >= 4:\n",
    "                    quad_ratings_binary.append(1)\n",
    "                else:\n",
    "                    quad_ratings_binary.append(0)\n",
    "        if SVM:\n",
    "            return freq_quad_temp, quad_ratings_binary\n",
    "        else:\n",
    "            return freq_quad_temp, quad_ratings\n",
    "    \n",
    "    def formatQuad(data_2=None, SVM=False):\n",
    "        import numpy as np\n",
    "        \n",
    "        if data_2 == None:\n",
    "            freq_quad_temp = freqQuad()\n",
    "        else:\n",
    "            def freqQuad(d):\n",
    "                class_per_quadrant = {}\n",
    "                temp = list(d.values())\n",
    "                for i in range(len(temp)):\n",
    "                    temp_data_holder = [{},{},{},{}]\n",
    "                    temp_data_holder_2 = []\n",
    "                    temp_image_name = image_names[i]\n",
    "                    temp_image = cv2.imread(directory + temp_image_name + '.png')\n",
    "                    w = temp_image.shape[0]\n",
    "                    h = temp_image.shape[1]\n",
    "                    for element in temp[i]:\n",
    "                        center_X = (element[0]+element[2])/2\n",
    "                        center_Y = (element[1]+element[3])/2\n",
    "                        if center_X >= w/2 and center_Y >= h/2:\n",
    "                            if temp_data_holder[0].get(int(element[5])) == None:\n",
    "                                temp_data_holder[0][int(element[5])] = 1\n",
    "                            else:\n",
    "                                temp_data_holder[0][int(element[5])] = temp_data_holder[0][int(element[5])] + 1\n",
    "                        if center_X < w/2 and center_Y >= h/2:\n",
    "                            if temp_data_holder[1].get(int(element[5])) == None:\n",
    "                                temp_data_holder[1][int(element[5])] = 1\n",
    "                            else:\n",
    "                                temp_data_holder[1][int(element[5])] = temp_data_holder[1][int(element[5])] + 1\n",
    "                        if center_X < w/2 and center_Y < h/2:\n",
    "                            if temp_data_holder[2].get(int(element[5])) == None:\n",
    "                                temp_data_holder[2][int(element[5])] = 1\n",
    "                            else:\n",
    "                                temp_data_holder[2][int(element[5])] = temp_data_holder[2][int(element[5])] + 1\n",
    "                        if center_X >= w/2 and center_Y < h/2:\n",
    "                            if temp_data_holder[3].get(int(element[5])) == None:\n",
    "                                temp_data_holder[3][int(element[5])] = 1\n",
    "                            else:\n",
    "                                temp_data_holder[3][int(element[5])] = temp_data_holder[3][int(element[5])] + 1\n",
    "                    for quadrant in temp_data_holder:\n",
    "                        for i in range(8):\n",
    "                            if quadrant.get(i) == None:\n",
    "                                temp_data_holder_2.append(0)\n",
    "                            else:\n",
    "                                temp_data_holder_2.append(quadrant[i])\n",
    "                    class_per_quadrant[int(temp_image_name)] = temp_data_holder_2\n",
    "                return class_per_quadrant\n",
    "            freq_quad_temp = freqQuad(raw_data)\n",
    "        freq_quad = {}\n",
    "        quad_ratings = []\n",
    "        quad_ratings_binary = []\n",
    "\n",
    "        for key, value in freq_quad_temp.items():\n",
    "            if not data.get(key) == None:\n",
    "                freq_quad[key] = value\n",
    "                quad_ratings.append(data[key])\n",
    "                if data[key] >= 4:\n",
    "                    quad_ratings_binary.append(1)\n",
    "                else:\n",
    "                    quad_ratings_binary.append(0)\n",
    "        if SVM:\n",
    "            return freq_quad, quad_ratings_binary\n",
    "        else:\n",
    "            return freq_quad, quad_ratings\n",
    "    #avgPosition,quadrants,classfreq,raw,modeltype,\n",
    "    if avgPosition:\n",
    "        aP = formatAvgPos(data_2=raw_data,SVM=False)[0]\n",
    "        for key, value in aP.items():\n",
    "            if testing.get(key) == None:\n",
    "                testing[key] = value\n",
    "            else:\n",
    "                testing[key].extend(value)\n",
    "    if quadrants:\n",
    "        q = formatQuad(data_2=raw_data,SVM=False)[0]\n",
    "        for key, value in q.items():\n",
    "            if testing.get(key) == None:\n",
    "                testing[key] = value\n",
    "            else:\n",
    "                testing[key].extend(value)\n",
    "    if classfreq:\n",
    "        f = formatFreq(data_2=raw_data,SVM=False)[0]\n",
    "        for key, value in f.items():\n",
    "            if testing.get(key) == None:\n",
    "                testing[key] = value\n",
    "            else:\n",
    "                testing[key].extend(value)\n",
    "    if raw:\n",
    "        r = formatRaw(data_2=raw_data,SVM=False)[0]\n",
    "        for key, value in r.items():\n",
    "            if testing.get(key) == None:\n",
    "                testing[key] = value\n",
    "            else:\n",
    "                testing[key].extend(value)\n",
    "    model_type = modeltype\n",
    "    model = mixFeatures(avgPosition,quadrants,classfreq,raw,model_type,False,use_independent_data=shuffle,shuffle=shuffle)\n",
    "    predictions_temp = model.predict(list(testing.values()))\n",
    "    predictions = []\n",
    "    #print(predictions_temp)\n",
    "    for prediction in predictions_temp:\n",
    "        if model_type == \"SVM\":\n",
    "            predictions.append(prediction)\n",
    "        else:\n",
    "            predictions.append(prediction[0])\n",
    "    actual = []\n",
    "    for key in list(testing.keys()):\n",
    "        actual.append(data[key])\n",
    "    def removeBadValues(act, pred):\n",
    "        act_2 = act.copy()\n",
    "        pred_2 = pred.copy()\n",
    "        count_removed = 0\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i]<0:\n",
    "                pred_2.pop(i-count_removed)\n",
    "                act_2.pop(i-count_removed)\n",
    "                count_removed+=1\n",
    "        return act_2, pred_2\n",
    "    actual, predictions = removeBadValues(actual, predictions)\n",
    "\n",
    "    if model_type == \"SVM\":\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] >= 4:\n",
    "                actual[i] = 1\n",
    "            else:\n",
    "                actual[i] = 0\n",
    "\n",
    "    accuracy = -1\n",
    "\n",
    "    if model_type == \"SVM\":\n",
    "        correct = 0\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == predictions[i]:\n",
    "                correct = correct + 1\n",
    "        accuracy = float(correct/len(actual))\n",
    "    else:\n",
    "        accuracy = mean_squared_error(actual, predictions)\n",
    "\n",
    "    return predictions, actual, accuracy\n",
    "\n",
    "# False, False, True, False is broken \"TypeError: float() argument must be a string or a real number, not 'dict'\"\n",
    "# True, True, False, False and True, False, False, False are both 0.64\n",
    "# False, True, False, False is 0.76\n",
    "\n",
    "mixValid(\"small_test/\",False,True,False,False,\"SVM\",False,shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
